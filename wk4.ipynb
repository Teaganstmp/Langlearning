{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teaganstmp/Langlearning/blob/main/wk4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG6xoh4Ae-D7"
      },
      "source": [
        "# LIN 393: Computional methods in linguistics, Fall 2024, Mahowald/Erk\n",
        "\n",
        "# Problem set 4: Dictionaries, functions, and text processing\n",
        "\n",
        "## Due: Tuesday Sep 24, end of day\n",
        "\n",
        "\n",
        "Please record all your answers in the appropriate place in this notebook. If you are doing the homework on Colab, then download the finished problem set using File -> Download -> Download .ipynb, and upload the ipynb file on Canvas.\n",
        "\n",
        "For the part of the homework that requires you to write Python code,\n",
        "we need to see the code, but not necessarily the output that the code\n",
        "produced (unless the problem explicitly asks for it).\n",
        "You can omit statements that\n",
        "produced an error or that did not form part of the eventual solution,\n",
        "but please include all the Python code that formed part of your\n",
        "solution.\n",
        "\n",
        "### Important note: Before you do anything else, please select \"Run all\", so the code included in this notebook will be executed in your copy of the notebook.\n",
        "\n",
        "\n",
        "**If any of these instructions do not make sense to you, please get in\n",
        " touch with the instructor right away.**\n",
        "\n",
        "\n",
        "A perfect solution to this homework will be worth *100* points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F68107tDN0yo"
      },
      "source": [
        "# Problem 1: List comprehensions (10 pts.)\n",
        "\n",
        "Here is a list of part-of-speech tags. Please use a list comprehension to transform this list into another list that only retains part-of-speech tags of content words, that is, part-of-speech tags that start with \"VB\", \"NN\", \"JJ\", or \"RB\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyOUSJ89N0yq"
      },
      "outputs": [],
      "source": [
        "taglist = ['DT', 'JJ', 'VBN', 'RB', 'IN', 'PRP$', 'JJ', 'NN', '.']\n",
        "# space for your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxoIblfrN0yq"
      },
      "source": [
        "Now we extend the problem: We start with a sentence, namely,\n",
        "\n",
        "    \"The bear apologized sincerely for his silly mistake.\"\n",
        "    \n",
        "Please use NLTK's word tokenizing function to split the sentence into words, then use NLTK's part-of-speech tagger to obtain a list of word-tag pairs. (See the Preprocessing_text notebook for this.)\n",
        "\n",
        "Then use a list comprehension to transform the list of word-tag pairs into a list that only retains word-tag pairs where the word is a content word, that is, where the tag starts in \"VB\", \"NN\", \"JJ\" or \"RB\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kqsBd6ON0yr"
      },
      "outputs": [],
      "source": [
        "sentence = \"The bear apologized sincerely for his silly mistake.\"\n",
        "# space for your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbb6Ltnp2CRv"
      },
      "source": [
        "# Problem 2: Python functions (30 pts.)\n",
        "\n",
        "For this problem, you'll write a few simple functions\n",
        "\n",
        "## 2a. Maximum of three numbers\n",
        "\n",
        "Write a Python function `max_of_three()` that takes as input three numbers (as three separate arguments), and returns the maximum of those three numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYNIn8A3PWmz"
      },
      "outputs": [],
      "source": [
        "# space for your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpbPMyLAPTV3"
      },
      "source": [
        "## 2b. POS list\n",
        "\n",
        "Write a Python function `to_postags()` that takes as input a string containing a single sentence, uses `nltk.word_tokenize()` to split the string into words, then uses `nltk.pos_tag()` to apply part-of-speech tagging to the list of words. Then the function should return *only the sequence of POS-tags* as a list, omitting the words.\n",
        "\n",
        "(About the string being a single sentence: You don't have to test whether it is indeed a single sentence; this just means that you expect a single sentence, so you do not split a text into sentences first.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBbHY5_DN0ys"
      },
      "outputs": [],
      "source": [
        "# space for your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifCWbrBLN0ys"
      },
      "source": [
        "## 2c. POS list as an iterator\n",
        "\n",
        "Write a Python function `each_postag()` that is just like `to_postags()` but that yields the part-of-speech tags instead of returning them, such that you can use it to iterate over POS tags. Your function should *not* produce a list of POS-tags, but yield each POS-tag in turn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAZ-Two9N0yt"
      },
      "outputs": [],
      "source": [
        "# space for your code here\n",
        "\n",
        "# then you should be able to do the following:\n",
        "# uncomment to test your code.\n",
        "#\n",
        "# for postag in each_postag(\"This is a test sentence.\"):\n",
        "#     print(postag)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47RDZAFYN0yt"
      },
      "source": [
        "# Problem 3: Preprocessing (25 pts.)\n",
        "\n",
        "For this problem, write a function `spacy_preprocess()` that uses spaCy instead of NLTK for part-of-speech tagging.\n",
        "\n",
        "The function should:\n",
        "* take as input a string containing one or more sentences\n",
        "* use spaCy to:\n",
        "    * split that text into sentences\n",
        "    * transform each sentence to a list of pairs of (word, part_of_speech_tag)\n",
        "* and return a list of sentences, where each sentence is a list of (word, part_of_speech_tag) pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUZLI0zSN0yt"
      },
      "outputs": [],
      "source": [
        "# space for your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av_BH0LhN0yu"
      },
      "source": [
        "# Problem 4: Counting words and part-of-speech tags (15 pts.)\n",
        "\n",
        "For this problem, you will use your spaCy-based part-of-speech tagger from Problem 3 on the short story \"A cask of Amontillado\" by E.A. Poe. The story is available on Canvas along with this homework, and on UTBox at https://utexas.box.com/s/f7py5ce11rg1ls5h45yp6hmrsaqis7hh.\n",
        "\n",
        "To use this data, you can do one of the following:\n",
        "\n",
        "\n",
        "*   Copy the text into a Python string, with triple quotes on either side. (It is not a long story, so you can do that.)\n",
        "*   Copy the file to your colab storage using\n",
        "```\n",
        "curl -L https://utexas.box.com/shared/static/f7py5ce11rg1ls5h45yp6hmrsaqis7hh\n",
        "```\n",
        "* Possibly also, upload the file to your colab storage through the web interface, though I am not sure about that.\n",
        "\n",
        "\n",
        "\n",
        " If you did not complete problem 3, use the following code to obtain NLTK part-of-speech tags instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS1rr6ZpN0yu",
        "outputId": "64629165-8903-4fa0-9635-b8fca86c50c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# code for turning a text into a list of sentences,\n",
        "# where each sentence is a list of word/tag pairs\n",
        "# use this only if you didn't complete problem 3.\n",
        "def nltk_preprocess(textstring):\n",
        "    output = [ ]\n",
        "    for sent in nltk.sent_tokenize(textstring):\n",
        "        output.append(nltk.pos_tag(nltk.word_tokenize(sent)))\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpzw6aoNN0yu",
        "outputId": "b12aad4c-6472-4dfd-a4f2-d73a3f2e8e51"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sentence', 'NN'), ('.', '.')],\n",
              " [('As', 'IN'), ('is', 'VBZ'), ('this', 'DT'), ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "nltk_preprocess(\"This is a sentence. As is this.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggqsNtsfN0yu"
      },
      "source": [
        "If you did finish problem 3, use your function `spacy_preprocess()` instead of the function `nltk_preprocess`  that I defined above.\n",
        "\n",
        "Use your preprocessing function, along with the NLTK data structure nltk.FreqDist,  to answer the following question:\n",
        "\n",
        "What is the most frequent word, and what is the most frequent POS-tag in \"The cask of Amontillado\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8NmGJFgN0yv"
      },
      "source": [
        "# Problem 5: Words that appear with more than one part of speech (20 pts.)\n",
        "\n",
        "For this problem, you will again use \"The cask of Amontillado\", and the part-of-speech tags you have computed for the words in the story.\n",
        "\n",
        "Which many words are ambiguous, in the sense that they appear with at least two different POS tags?\n",
        "\n",
        "To solve this problem, do not use the NLTK data structures nltk.FreqDist and nltk.ConditionalFreqDist, nor the Python Counter. Just use Python dictionaries. But you can (and may want to) use Python's `defaultdict`.\n",
        "\n",
        "Make a dictionary that stores, for each word, the part-of-speech tags with which you observe it to occur in the story. Note that you only want distinct tags: If a word has appeared with `\"NN\", \"NN\", \"NN\"`, it is not ambiguous. But if it has appeared with\n",
        "`\"NP\", \"NN\", \"NN\", \"NN\", \"NP\"`, then it is ambiguous. There are multiple ways to go about this. You can either make sure that you only save tags that you have not before seen with the word in question. Or you can store the full list of POS tags with duplicates, and then remove duplicates before you count POS tags. Or you can store POS tags as sets instead of lists. A set is similar to a list, but it never contains duplicates.\n",
        "You add to a set using `add()` rather than `append()` as for lists:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of Python sets\n",
        "mylist = [\"NP\", \"NP\", \"NN\", \"NN\", \"NN\", \"NP\"]\n",
        "myset = set(mylist)\n",
        "print(\"mylist converted to a set gives me:\", myset)\n",
        "myset.add(\"NNS\")\n",
        "print(\"after adding 'NNS', I got: \", myset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leUqRrVAhMS1",
        "outputId": "7a009810-2e7d-45de-f457-12cfedb4f581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mylist converted to a set gives me: {'NN', 'NP'}\n",
            "after adding 'NNS', I got:  {'NN', 'NNS', 'NP'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# space for your code here"
      ],
      "metadata": {
        "id": "0Bv0_2VlhdGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}