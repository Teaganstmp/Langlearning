{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teaganstmp/Langlearning/blob/main/Copy_of_wk3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG6xoh4Ae-D7"
      },
      "source": [
        "# LIN 393: Computional methods in linguistics, Fall 2024, Mahowald/Erk\n",
        "\n",
        "# Problem set 3, version for beginning programmers: Conditions, lists, and loops\n",
        "\n",
        "## Due: Tuesday Sep 17, end of day\n",
        "\n",
        "\n",
        "Please record all your answers in the appropriate place in this notebook. If you are doing the homework on Colab, then download the finished problem set using File -> Download -> Download .ipynb, and upload the ipynb file on Canvas.\n",
        "\n",
        "For the part of the homework that requires you to write Python code,\n",
        "we need to see the code, but not necessarily the output that the code\n",
        "produced (unless the problem explicitly asks for it).\n",
        "You can omit statements that\n",
        "produced an error or that did not form part of the eventual solution,\n",
        "but please include all the Python code that formed part of your\n",
        "solution.\n",
        "\n",
        "### Important note: Before you do anything else, please select \"Run all\", so the code included in this notebook will be executed in your copy of the notebook.\n",
        "\n",
        "\n",
        "**If any of these instructions do not make sense to you, please get in\n",
        " touch with the instructor right away.**\n",
        "\n",
        "\n",
        "A perfect solution to this homework will be worth *100* points.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBLULGnQe-EI"
      },
      "source": [
        "# Problem 1: Removing punctuation, with loops (30 pts.)\n",
        "\n",
        "\n",
        "Here is a text passage from  R.L. Stevenson, *Treasure Island* (from Project Gutenberg, https://www.gutenberg.org/ebooks/120):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GRqbmDne-EK"
      },
      "outputs": [],
      "source": [
        "treasureisland_text = \"\"\"The supervisor stood up straight and stiff and\n",
        "told his story like a lesson; and you should have seen how the two\n",
        "gentlemen leaned forward and looked at each other, and forgot to smoke\n",
        "in their surprise and interest. When they heard how my mother went\n",
        "back to the inn, Dr. Livesey fairly slapped his thigh, and the squire\n",
        "cried “Bravo!” and broke his long pipe against the grate.\n",
        "Long before it was done, Mr. Trelawney (that, you will remember,\n",
        "was the squire’s name) had got up from his seat and was striding about\n",
        "the room, and the doctor, as if to hear the better, had taken off his\n",
        "powdered wig and sat there looking very strange indeed with his own\n",
        "close-cropped black poll.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7ED8FfKe-EP"
      },
      "source": [
        "For this problem, please remove punctuation from beginnings and ends of words in the passage by using loops. Please proceed as follows:\n",
        "\n",
        "* Please split the passage into words using Python's ```split()```. Do not use NLTK's ```word_tokenize()```, as that already does the work of removing punctuation for you.\n",
        "\n",
        "* Then run a for-loop over the list of words, and use Python's ```strip()``` method to remove punctuation. You will need to give `strip()` an appropriate parameter to do this. Collect the resulting list of words without punctuation in a separate list. (That is, use the \"aggregation\" programming idiom that we discussed in class.) You do not need to keep the punctuation in a separate string. For this problem please just You do not need to remove punctuation from the middle of strings, that is, you do not need to transform the string *squire's* in any way.\n",
        "\n",
        "Technically, the period at the end of the title *Dr.* is not a punctuation symbol, but you can ignore that for now, and just remove it anyway.\n",
        "\n",
        "Here is information about the method ```strip()```: https://docs.python.org/3.8/library/stdtypes.html#str\n",
        "\n",
        "Also note that Python has a string full of punctuation symbols that might be useful for this problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isjooHB2e-EQ",
        "outputId": "668dd1e7-cb7c-4e2e-daf9-b3faf8a70be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDBCLqLde-ES"
      },
      "outputs": [],
      "source": [
        "# place your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERfcQdoqe-ET"
      },
      "source": [
        "# Problem 2: Stemming (30 pts.)\n",
        "\n",
        "Stemming is the process of mapping word forms to an approximation of their base form, the stem. Stemming is a simplified, heuristic version of lemmatization. Stemming or lemmatization is often useful as a preprocessing step, for example when you want to count how often each word appears in a text, and you don't want to count different word forms separately. For more information see https://en.wikipedia.org/wiki/Stemming\n",
        "\n",
        "For this problem, you will implement a simple stemmer. Here is a text passage, from C. S. Lewis' *The Lion, the Witch, and the Wardrobe*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF2GFhnse-ET"
      },
      "outputs": [],
      "source": [
        "lewistext = \"\"\"And Peter with his sword still drawn in his hand went with\n",
        "the Lion to the eastern edge of the hill-top.  There a beautiful\n",
        "sight met their eyes.  The sun was setting behind their backs.\n",
        "That meant that the whole country below them lay in the evening\n",
        "light--forest and hills and valleys and, winding away like a\n",
        "silver snake, the lower part of the great river.  And beyond\n",
        "all this, miles away, was the sea, and beyond\n",
        "the sea the sky, full of clouds which were just turning rose colour\n",
        "with the reflection of the sunset.  But just where the land of Narnia\n",
        "met the sea--in fact, at the mouth of the great river--there was\n",
        "something on a little hill, shining.  It was shining because it was a\n",
        "castle and of course the sunlight was reflected from all the windows\n",
        "which looked towards Peter and the sunset; but to Peter it looked like\n",
        "a great star resting on the seashore.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCv7Loaze-EU"
      },
      "source": [
        "For this problem, please perform stemming on this text, as follows:\n",
        "    \n",
        "* Split the text into words, this time using ```nltk.word_tokenize()```. This will split off the punctuation into separate strings. The result is a list of words.\n",
        "\n",
        "* Iterate over the words in the list, perform stemming on each one (or leave it the same): Please remove word endings that are \"ing\", \"ed\", \"ion\", or \"s\". Put the resulting string in a new list. After the for-loop, the result should be a list of words exactly as long as the original word list, but stemmed. So this is another instance of the \"aggregation\" programming idiom we discussed in class.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qQ5ePCMe-EW",
        "outputId": "04ffb21b-4937-4d39-c015-078a7c49f1a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# You'll need to re-download the NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zy1ysmLue-EX"
      },
      "outputs": [],
      "source": [
        "# space for your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLOFy8Yye-EY"
      },
      "source": [
        "# Problem 3: counting part-of-speech tags (40 pts.)\n",
        "\n",
        "\n",
        "For this problem, we'll work with the Brown corpus. The Brown corpus is a collection of text pieces from different domains, selected to be about balanced in size across different text types. It comprises about 1 million words. Here is more information on the Brown corpus: https://en.wikipedia.org/wiki/Brown_Corpus\n",
        "\n",
        "We'll use the *news* part of the Brown Corpus in the Natural\n",
        "Language Toolkit, specifically a version of this corpus with\n",
        "part-of-speech tags (POS tags)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "womNfOmxe-EY",
        "outputId": "46d5ac6a-058c-4d7c-9aa0-087c04f252bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('brown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e6zH15ke-EZ"
      },
      "source": [
        "Here is how you can access the Brown news section as a list of words. We'll look at the first 20 of them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "co9Ya4FNe-Ea",
        "outputId": "ff7686cc-cd25-40b5-8079-456a4a889e3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'Fulton',\n",
              " 'County',\n",
              " 'Grand',\n",
              " 'Jury',\n",
              " 'said',\n",
              " 'Friday',\n",
              " 'an',\n",
              " 'investigation',\n",
              " 'of',\n",
              " \"Atlanta's\",\n",
              " 'recent',\n",
              " 'primary',\n",
              " 'election',\n",
              " 'produced',\n",
              " '``',\n",
              " 'no',\n",
              " 'evidence',\n",
              " \"''\",\n",
              " 'that']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "nltk.corpus.brown.words(categories=\"news\")[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TKXPjnje-Eb"
      },
      "source": [
        "And here is how you can access the Brown news section as a list of **words with their parts of speech**. We'll again look at the first 20 words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QiApaSNee-Ed",
        "outputId": "01631afd-803d-4d06-a1ad-ba3467950adb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'AT'),\n",
              " ('Fulton', 'NP-TL'),\n",
              " ('County', 'NN-TL'),\n",
              " ('Grand', 'JJ-TL'),\n",
              " ('Jury', 'NN-TL'),\n",
              " ('said', 'VBD'),\n",
              " ('Friday', 'NR'),\n",
              " ('an', 'AT'),\n",
              " ('investigation', 'NN'),\n",
              " ('of', 'IN'),\n",
              " (\"Atlanta's\", 'NP$'),\n",
              " ('recent', 'JJ'),\n",
              " ('primary', 'NN'),\n",
              " ('election', 'NN'),\n",
              " ('produced', 'VBD'),\n",
              " ('``', '``'),\n",
              " ('no', 'AT'),\n",
              " ('evidence', 'NN'),\n",
              " (\"''\", \"''\"),\n",
              " ('that', 'CS')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "nltk.corpus.brown.tagged_words(categories=\"news\")[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aUcixaCe-Ed"
      },
      "source": [
        "As you can see, we now have a list of tuples, where each tuple represents a word. Reminder: A tuple is a data structure that is exactly like a list, except that it is non-mutable, that is, there is no `append()` or other method that changes data in place.\n",
        "\n",
        "Each tuple contains two elements, both of them strings: first the word, then the part-of-speech tag. The tagset used here is coarse-grained and fine-grained at the same time: The first two letters of the part-of-speech tag are a coarse-grained tag, such as \"NN\" for noun, or \"VB\" for verb. All the remaining letters give you a fine-grained part-of-speech tag: \"VBD\" is a verb that is in past tense, and \"NN-TL\" is a noun that is part of a title.\n",
        "\n",
        "For this problem, please count how many common nouns there are in the Brown news section, that is, how many words have a part-of-speech tag that starts with \"NN\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "al9jBd_3e-Ee"
      },
      "outputs": [],
      "source": [
        "# space for your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Advanced Students: Problem 4. Finding Haikus Using NLTK\n",
        "\n",
        "There is a fun Twitter account that uses a script to find sentences which accidentaly form \"haikus\" in the New York Times https://x.com/nythaikus.\n",
        "\n",
        "This sentence, for one, can become a haiku if you just add line breaks.\n",
        "\n",
        "\n",
        "\n",
        "> This sentence, for one,\n",
        "\n",
        "\n",
        "> can become a haiku if\n",
        "\n",
        "> you just add line breaks.\n",
        "\n",
        "\n",
        "\n",
        "Your job is to find sentences which are haikus, for some corpus of your choosing from NLTK. (This will work with the Brown corpus, but you should fel free to play around with other corpora or even your own data set from the internet.)\n",
        "\n",
        "A [Haiku](https://en.wikipedia.org/wiki/Haiku) consists of 3 lines, of 5, 7, and 5 syllables respectively. You need to find the sentences that can be broken up into this format, and then output them in 3 lines.\n",
        "\n",
        "To find out how many syllables are in a word, use the CMU Pronunciation Dictionary, which is available in nltk. Read the documentation to get a sense of how to use it! Below, we include a function that takes the pronunciations and looks for stressed vowel phonemes, which is a decent cue in the CMU pronunciation dictionary for syllables (but not perfect). Feel free to try to refine it if you notice errors in the evaluation below!\n",
        "\n",
        "Some considerations for you to think about:\n",
        "- How will you handle punctuation?\n",
        "- How will you handle words not in the CMU pronunciation dictionary?\n",
        "- You should first play around with the CMU pronunciation dictionary to understand what kind of format it likes (should you break up contractions? You might need to think about the tokenizer).\n",
        "- OPTIONAL BONUS: A nice thing to do, if you are so inclined, would be to design your system to be general enough so that you could take in *any* syllable pattern in some format and return sentences that match that format. Let's say a list of syllable counts like `[5, 7, 5]` for haikus but you might also choose to instead pass in `[5, 7, 5, 7, 7]` for a [tanka](https://en.wikipedia.org/wiki/Tanka)."
      ],
      "metadata": {
        "id": "OHJ7I6mEfFSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown, cmudict\n",
        "\n",
        "# Ensure the required resources are downloaded\n",
        "nltk.download('brown')\n",
        "nltk.download('cmudict')\n",
        "\n",
        "# Load the Brown corpus\n",
        "# use this or use your own corpus\n",
        "brown_sents = brown.sents()\n",
        "\n",
        "# Load the CMU pronouncing dictionary\n",
        "pronouncing_dict = cmudict.dict()\n",
        "\n",
        "def get_syllable_count(word):\n",
        "  \"\"\"Return the number of non-alpha characters in a CMU\n",
        "  pronunciation. These characters correspond to stressed\n",
        "  or unstressed vowels (1 for stressed, 0 for unstressed).\n",
        "  See CMU documentation for more.\"\"\"\n",
        "  first_pronunciation = pronouncing_dict[word][0]\n",
        "  return len([i for i in first_pronunciation if not i.isalpha()])\n",
        "\n",
        "# see if the get_sylalble_count() function is doing something reasonable\n",
        "# assertions won't do anything if the assertion is true\n",
        "assert get_syllable_count(\"test\") == 1\n",
        "assert get_syllable_count(\"establishment\") == 4\n",
        "assert get_syllable_count(\"python\") == 2\n",
        "\n",
        "# HINT: to avoid errors, you can use a try/except\n",
        "newword = \"crazymadeupword\"\n",
        "try:\n",
        "  get_syllable_count(newword)\n",
        "except KeyError:\n",
        "  print (f\"I guess that {newword} isn't in the CMU dictionary\")\n",
        "\n",
        "# TODO: your code goes here\n",
        "# you should print out all the haikus you find in your corpus"
      ],
      "metadata": {
        "id": "3ErtwTNtfE0F",
        "outputId": "d1f1a313-ce45-4ba0-f949-7bbff4f3a5cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I guess that crazymadeupword isn't in the CMU dictionary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating your haikus\n",
        "\n",
        "After you have printed your haikus, tell us which one was your favorite!\n",
        "\n",
        "Then, randomsly sample 10 haikus (if you have 10), and count up how many of the 10 are \"good haikus\". The proportion that are good is what in NLP we would call the \"precision\" of your system. Checking how many out of 10 are good can give us a quick estimate of our precision.\n",
        "\n",
        "There is a related quantity that we call your \"recall\": this would, in this case, be the proportion of \"real haikus\" in the corpus that you successfully detected. This is impossible to count in this case, since we would need to know which haikus your system missed!\n",
        "\n",
        "OPTIONAL BONUS: Above we asked for your favorite haiku. Think a bit about what made it your favorite. Could you think of ways to try to pick out automatically what separates your favorite haikus from the less good ones?"
      ],
      "metadata": {
        "id": "dOWmm1U-vhg7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbb6Ltnp2CRv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}