{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Teaganstmp/Langlearning/blob/main/Making_complex_objects_with_their_own_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4505be97-a143-4b7b-badd-d3a5fb836394",
      "metadata": {
        "id": "4505be97-a143-4b7b-badd-d3a5fb836394"
      },
      "source": [
        "# Classes in Python: complex data types\n",
        "\n",
        "Sometimes the data that we want to store is complex. You've seen an example in the Universal Dependencies, where each word was represented as its form, lemma, part of speech, and incoming dependency arc. As another example, you may need to store participants in an experiment with their anonymized ID, age, and Likert scale ratings (say, on a scale from 1-5) that they gave for each experimental items.\n",
        "\n",
        "For the Universal Dependencies case, we've stored each token as a Python dictionary representing an attribute-value matrix.\n",
        "\n",
        "But Python has another option for storing complex data: You can define your own data type: You define a new *class* of data, and then you can make individual pieces of data as *objects* of that class. The advantage in that case is that you can also define your own *methods* for how to interact with that data type.\n",
        "\n",
        "Here is an example of a new class for storing Universal Dependencies AVM information. As a reminder, here is the AVM for the first word of the 10th sentence of the the UD_English-GUM corpus:\n",
        "\n",
        "\n",
        "$$\n",
        "\\left[\\begin{array}{ll}\n",
        "\\text{id:} & 1\\\\\n",
        "\\text{form:} & 'Thus'\\\\\n",
        "\\text{lemma:} & 'thus'\\\\\n",
        "\\text{upos:} &  'ADV'\\\\\n",
        "\\text{xpos:} & 'RB'\\\\\n",
        "\\text{feats:} &  None\\\\\n",
        "\\text{head:} & 16\\\\\n",
        "\\text{deprel:}  & advmod\\\\\n",
        "\\text{deps:}  & None\\\\\n",
        "\\text{misc:} & \\left[\\begin{array}{ll}\n",
        "\\text{SpaceAfter:} & 'No'\n",
        "\\end{array}\\right]\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "We'll simplify it a bit for this example:\n",
        "\n",
        "\n",
        "$$\n",
        "\\left[\\begin{array}{ll}\n",
        "\\text{id:} & 1\\\\\n",
        "\\text{form:} & 'Thus'\\\\\n",
        "\\text{lemma:} & 'thus'\\\\\n",
        "\\text{upos:} &  'ADV'\\\\\n",
        "\\text{head:} & 16\\\\\n",
        "\\text{deprel:}  & advmod\\\\\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "Here is the Universal Dependencies AVM class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8735dcf2-2c18-4711-b518-253e3360a95b",
      "metadata": {
        "tags": [],
        "id": "8735dcf2-2c18-4711-b518-253e3360a95b"
      },
      "outputs": [],
      "source": [
        "class UDavm:\n",
        "    def __init__(self, word_id, form, lemma, upos, head, deprel): # defiine the object with the boject you use it with, i.e. self\n",
        "        self.word_id = word_id\n",
        "        self.form = form\n",
        "        self.lemma = lemma\n",
        "        self.upos = upos\n",
        "        self.head = head\n",
        "        self.deprel = deprel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70700942-cab9-4735-a478-1838009bf6a1",
      "metadata": {
        "id": "70700942-cab9-4735-a478-1838009bf6a1"
      },
      "source": [
        "This makes a new class of objects, each of which have attributes `word_id`, `form`, `lemma`, `upos`, `head`,  and`deprel`.\n",
        "\n",
        "We have a new reserved word, `class`, which is formed by a class name (a variable name), and a colon.\n",
        "The class has one method, with the odd name `__init__`. This is the method that gets called when you make a new object of this class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "665ebc35-a509-48bb-b8d4-af3510742a85",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "665ebc35-a509-48bb-b8d4-af3510742a85",
        "outputId": "cbaa0256-d5b1-481e-bdff-5cf23265b708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thus\n",
            "advmod\n"
          ]
        }
      ],
      "source": [
        "# This makes an object of class UDavm\n",
        "avmobj = UDavm(1, \"Thus\", \"thus\", \"ADV\", 16, \"advmod\")\n",
        "\n",
        "# we can now access the attributes in an expression\n",
        "# <variable holding the object> . <attributename>\n",
        "print(avmobj.form)\n",
        "print(avmobj.deprel)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f820f2-25d5-4be8-ae9d-6546d831feaf",
      "metadata": {
        "id": "09f820f2-25d5-4be8-ae9d-6546d831feaf"
      },
      "source": [
        "When we make the object `avmobj`, the method `__init__()` of the class gets called. It stores values for all the attributes -- but where? Note that the method is defined with one more argument that how we call it: there is an additional `self`. `self` is the object itself. When we state `self.form = form`, we store the value of `form` in an attribute `form` that is attached to the object.\n",
        "\n",
        "Here is another object, with different values in its AVM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "425749b1-56fd-44b8-bfbd-9fb1edb4edf9",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "425749b1-56fd-44b8-bfbd-9fb1edb4edf9",
        "outputId": "65bef6b5-8f1c-4ae1-fa6d-7ccb85ac6206"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'visually'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "avmobj2 = UDavm(1, \"visually\", \"visually\", \"ADV\", 12, \"advmod\")\n",
        "\n",
        "avmobj2.form"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c21b67a-4f2c-491b-9c39-b1154490f41e",
      "metadata": {
        "id": "9c21b67a-4f2c-491b-9c39-b1154490f41e"
      },
      "source": [
        "Now let's define the class again, but add a method. Say we want to add functionality that looks up the syllable structure for the word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5025caa4-c8f5-4786-b4a1-9cb444b3eee9",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5025caa4-c8f5-4786-b4a1-9cb444b3eee9",
        "outputId": "56036233-29ab-4183-b400-35d028408f5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "\n",
        "class UDavm:\n",
        "    def __init__(self, word_id, form, lemma, upos, head, deprel):\n",
        "        self.word_id = word_id\n",
        "        self.form = form\n",
        "        self.lemma = lemma\n",
        "        self.upos = upos\n",
        "        self.head = head\n",
        "        self.deprel = deprel\n",
        "\n",
        "        # loading the CMU dictionary\n",
        "        self.cmudict = nltk.corpus.cmudict.dict()\n",
        "\n",
        "    def syllables(self):\n",
        "        first_pronunciation = self.cmudict[self.form][0]\n",
        "        return len([i for i in first_pronunciation if not i.isalpha()])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa5eed13-2bb5-467e-b092-a5765bb54341",
      "metadata": {
        "id": "aa5eed13-2bb5-467e-b092-a5765bb54341"
      },
      "source": [
        "The new method again has an argument `self`. This time, the body of the method uses `self`, specifically the dictionary and the word form stored in `self`: They were initialized in `__init__()` and then persisted.\n",
        "\n",
        "When we call the method, we again call it with one argument less than we defined it, in this case with zero arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6a24e34c-3c70-49f3-91f2-ebf4baa0acc4",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a24e34c-3c70-49f3-91f2-ebf4baa0acc4",
        "outputId": "b36f547e-26b7-4bad-e398-e58c45d8728d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# making an AVM object\n",
        "avmobj2 = UDavm(1, \"visually\", \"visually\", \"ADV\", 12, \"advmod\")\n",
        "\n",
        "# how many syllables does this word have?\n",
        "avmobj2.syllables()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12574f8a-77f5-4c6d-8e95-3942ca03fb0f",
      "metadata": {
        "id": "12574f8a-77f5-4c6d-8e95-3942ca03fb0f"
      },
      "source": [
        "# Classes in Python packages\n",
        "\n",
        "Many Python packages define their own data classes. For example, you have seen some classes that come with the Natural Language Toolkit.\n",
        "\n",
        "One of them is FreqDist, a dictionary with extra bells and whistles that counts how often each word appears in a given word list. Here it is, as a reminder:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8ea93909-6c04-49cf-ac1c-faed676200ba",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "8ea93909-6c04-49cf-ac1c-faed676200ba",
        "outputId": "c43e9e9b-1b30-4f13-e7d3-e67d93bfbd6b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-83b88762390e>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mit\u001b[0m \u001b[0magain\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0magain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \"\"\"\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "text = \"\"\"You are old, Father William,\" the young man said,\n",
        "    \"And your hair has become very white;\n",
        "And yet you incessantly stand on your head—\n",
        "    Do you think, at your age, it is right?\"\n",
        "\n",
        "\"In my youth,\" Father William replied to his son,\n",
        "    \"I feared it might injure the brain;\n",
        "But now that I'm perfectly sure I have none,\n",
        "    Why, I do it again and again.\"\n",
        "    \"\"\"\n",
        "fd = nltk.FreqDist(nltk.word_tokenize(text))\n",
        "fd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf088ae-1e7b-4197-8a0a-470c1a9690a0",
      "metadata": {
        "id": "4bf088ae-1e7b-4197-8a0a-470c1a9690a0"
      },
      "source": [
        "You can see the whole source code for FreqDist here: https://www.nltk.org/_modules/nltk/probability.html#FreqDist\n",
        "\n",
        "It starts with\n",
        "\n",
        "```\n",
        "class FreqDist(Counter):\n",
        "   ...\n",
        "```\n",
        "\n",
        "That means that it is a class that is *derived from* the Python data type `Counter`. It takes over all the methods and attributes of `Counter`, and to do the counting, it basically just hands all its data to `Counter`. Leaving out a lot of comments and some code, we have:\n",
        "\n",
        "```\n",
        "class FreqDist(Counter):\n",
        "    def __init__(self, samples=None):\n",
        "        Counter.__init__(self, samples)\n",
        "```\n",
        "\n",
        "One of the methods it defines is `hapaxes`, which returns a list of all the words that appeared only once. Here is its definition:\n",
        "\n",
        "```\n",
        "class FreqDist(Counter):\n",
        "    ...\n",
        "    def hapaxes(self):\n",
        "        return [item for item in self if self[item] == 1]\n",
        "```\n",
        "\n",
        "Here the self object can be accessed like a dictionary, an ability that  is inherited from `Counter`. The keys in this dictionary are words, and the values are their counts. As you can see, the method accesses all items in the `self` dictionary that have a count of one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d67c51-5eea-459a-9c7c-5807a26e4421",
      "metadata": {
        "id": "c6d67c51-5eea-459a-9c7c-5807a26e4421"
      },
      "source": [
        "## The function dir()\n",
        "\n",
        "The function `dir()` gives you access to all the attributes and methods stored with an object. Here it is applied to the FreqDist object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34fe3cae-b587-45ab-a767-8077d2de7e7a",
      "metadata": {
        "tags": [],
        "id": "34fe3cae-b587-45ab-a767-8077d2de7e7a"
      },
      "outputs": [],
      "source": [
        "dir(fd)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a15eb553-fe3c-43fa-82e6-a370029c286a",
      "metadata": {
        "id": "a15eb553-fe3c-43fa-82e6-a370029c286a"
      },
      "source": [
        "# Building a class that enables corpus access in NLTK\n",
        "\n",
        "Here is a self-made object type that gives access to a corpus, particularly one that is stored in the manner of nltk corpora.\n",
        "\n",
        "The `__init__()` method stores the directory name.\n",
        "\n",
        "The method `fileids()` does the same thing as the `fileids()` method for nltk corpora: It gives you the names of all the corpus pieces that you can access individually.\n",
        "\n",
        "The method `words()` does the same thing as the `words` method for nltk corpora: Given a file ID, it gives you the corpus piece stored under that file ID as a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08ce0565",
      "metadata": {
        "tags": [],
        "id": "08ce0565"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import nltk\n",
        "\n",
        "class MyCorpus:\n",
        "    # initialization\n",
        "    def __init__(self, directoryname):\n",
        "        self.directoryname = directoryname\n",
        "\n",
        "    # return list of file IDs:\n",
        "    # all filenames in the directory of the corpus\n",
        "    # that end in .txt\n",
        "    def fileids(self):\n",
        "        return [name for name in os.listdir(self.directoryname)\n",
        "                if name.endswith(\"txt\")]\n",
        "\n",
        "    # return list of words for one file ID\n",
        "    def words(self, fileid):\n",
        "        whole_filename = os.path.join(self.directoryname, fileid)\n",
        "        f = open(whole_filename)\n",
        "        contents = f.read()\n",
        "        f.close()\n",
        "        return nltk.word_tokenize(contents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we download NLTK data, it goes to \"/root/nltk_data/corpora\""
      ],
      "metadata": {
        "id": "DKXP2pcDH9BB"
      },
      "id": "DKXP2pcDH9BB"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"state_union\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "import os\n",
        "os.listdir(\"/root/nltk_data/corpora\")"
      ],
      "metadata": {
        "id": "JqX0DFFUHiUy"
      },
      "id": "JqX0DFFUHiUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb57400",
      "metadata": {
        "tags": [],
        "id": "2cb57400"
      },
      "outputs": [],
      "source": [
        "testcorpus = MyCorpus(\"/root/nltk_data/corpora/state_union\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "823ede2d",
      "metadata": {
        "tags": [],
        "id": "823ede2d"
      },
      "outputs": [],
      "source": [
        "testcorpus.directoryname"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84bd9522",
      "metadata": {
        "tags": [],
        "id": "84bd9522"
      },
      "outputs": [],
      "source": [
        "testcorpus.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b838dd0",
      "metadata": {
        "tags": [],
        "id": "1b838dd0"
      },
      "outputs": [],
      "source": [
        "firstfile = testcorpus.fileids()[0]\n",
        "testcorpus.words(firstfile)[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d460fe2",
      "metadata": {
        "id": "4d460fe2"
      },
      "outputs": [],
      "source": [
        "firstfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f148b7bc",
      "metadata": {
        "id": "f148b7bc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}